\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex	
\usepackage{array}
\usepackage{amssymb}
\usepackage{todonotes}

%SetFonts

%SetFonts


\title{User Preferences in Graphical and Textual Interfaces for Kubernetes}
\author{The Authors}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}

\section{Introduction}
%Tool developer culture has established two modalities (text and graphics): how can we be generative and constructive in combining these or offering a new modality in light of their limitations?
%The goal of our study is to illuminate user preferences for the flexibility tradeoffs offered by different classes of tools (textual and graphical) for Kubernetes. Furthermore, we seek to understand why users prefer the flexibility tradeoffs offered by different classes of tools in the context of specific interactions. 
% Define: perceived cognitive load, task switching, subtasks...
We seek to answer the following research questions and determine the validity of our hypotheses:

RQ1. How does tool preference vary by tool type, input type frequency, subtask, and perceived cognitive load?

H1. Preference correlates with input type frequency...

RQ2. How does perceived cognitive load vary by tool type, subtask, and input type switching frequency?

H2. Perceived cognitive load correlates with tool type...

\section{Related Work}
Effects of a hybrid interface and multimodal features for learning how to program \cite{unal2021effects, grafsgaard2014additive}.

Benefits of a textual versus graphical interfaces for learning how to program \cite{dillon2012comparing}.

Measuring perceived cognitive load using the NASA-TLX scale \cite{hart1988development}.

User evaluation methods and the impact of think-aloud on user testing \cite{mcdonald2020impact, ericsson1984protocol, fox2011procedures}. 

Task analysis on the challenges of shell programming \cite{gandhi2020lightening}.

Optimizing visual and textual information and predicting user frustration in search user interfaces \cite{treharne2012optimising, feild2010predicting}.
\todo[inline]{motivate relevance of each and illuminate open research avenues that stem from their findings}


\section{Study Setup}
We conducted a remotely moderated, within subjects user study with X participants who had an average of X years of programming experience. All participants belonged to the Hybrid Cloud research group at IBM and regularly used Kubernetes in their work. Since the purpose of our study is not to learn the startup cost of using a tool, we recruited participants that had some familiarity with the tools. 

Before the study sessions, participants filled out a background survey to provide information about their programming experience, use of Kubernetes, and familiarity with the two tools. After completing the study, participants filled out a follow up survey and ranked the tools based on their preference. In the follow up survey, participants were also asked to rate the tools based on their perceived cognitive load using the NASA-TLX scale. To give us a better understanding of preference, participants also rated the quality of each tool on a five-point Likert scale and were asked to give reasons for why they either liked or didn’t like using each tool. Participants were also asked to share their recommendations for how each tool could be improved. \todo[inline]{summarize the questions asked in the surveys in a more general/thematic way}

During the study sessions, participants completed a Kubernetes task using the kubectl CLI and the OpenShift console. The order in which participants used the tools was counter balanced to mitigate learning bias between sessions, with half of the participants using the kubectl CLI first and the other half using the OpenShift console first \cite{}. We employed a classic "Think Aloud" user evaluation method during the sessions to avoid the effects of reactivity \cite{}. Video and audio from the sessions were recorded for later transcription and analysis. 

\subsection{Pilot Testing}
We conducted pilot testing with two participants to identify any issues in our study design. One participant identified a misleading error in one of the task instructions so we rewrote it. Initially, we had only provided task hints for the kubectl CLI, and one participant realized that they could not apply these hints to the OpenShift console, so we also included task hints for the OpenShift console. We found that both participants were able to complete the task using each tool in under 20 minutes.

\subsection{Kubernetes Task}
The goal of the Kubernetes task is to create an application in a namespace, find the deployment pods that have the string “magic key” in their logs, delete the application, and then delete the namespace. This task was chosen for our study because it is simple enough to be accomplished by a Kubernetes novice in multiple steps or by an expert in fewer steps and it requires participants to perform search and navigation using each tool. Participants were given a GitHub repo with the application files and task instructions. If participants got stuck during the session, hints specific to each tool interface were provided in links below the task instructions. Participants were told to think aloud during the session and given a prompt to continue thinking aloud if they fell silent for more than 15 seconds.

\section{Evaluation Methodology}
\subsection{Low Level User Interactions}
A challenge we faced was in deciding what metrics to use to compare these tools. We sought to use a WhatPulse to measure physical user interaction frequencies and produce mouse movement heatmaps, however we observed errors in the numbers and decided not to use it \cite{}. Furthermore, to what extent do heatmaps let us compare between tools with different input methods? Heatmaps could be a good metric for A/B testing, however, they would be a poor metric for comparing different tools. We assume that these tools are already well designed and instead seek to study the limits that exist within the steady state of design and learning. These modalities will have certain limits and we seek to understand what are they as is to determine how we can improve them. 

Wall clock time is easy to measure and is objective, however, across these tools isn’t really comparable because of the user variance we observed in the participant workflows. For example, one participant used the "Topology" view to access the logs for each deployment's pods (shown in Figure X) while other participants used the list of pods found in the "Project Inventory" to access the logs (shown in Figure X). User preference is a better metric, however, it may be too subjective. For example, knowing people prefer a CLI for certain tasks doesn’t help to improve the browser console for those tasks. Therefore, to support the qualitative data gathered in our surveys and study sessions, we also collected low level user interaction data, specifically user input types (keys, clicks, and mouse movements) and how frequently the participant used each user input type in completing the task on both tool interfaces. 

The video recordings were manually analyzed to count how frequently participants used the keyboard to enter a command (k), how frequently participants clicked (c), and how frequently participants moved their mouse and/or scrolled to click (m). Participant input type frequencies were stored in tuples as (k, c, m). Rather than count characters to tally the keyboard input, we chose to count the number of total commands entered to normalize the input frequencies as we are neither counting the duration of the clicks nor the length of the mouse movements. In the OpenShift console, we counted commands as the number of times participants typed something followed by an enter or click to enter. Mouse movements were only noted when they were followed by a purposeful click (for example, moving the mouse to click on the browser back button), and otherwise random mouse movements were not counted in our analysis. 

\subsection{High Level User Interactions}
As we observed in our study sessions, participants do not consider these inputs as having equal execution difficulty. For example, while half of our participants copy/pasted the pod names from the list of pods in the terminal to get the logs for each pod, the other half used tab completion to get the pod names and execute the command to get the logs. Therefore, we also kept track of the specific commands used by participants, such as how frequently participants used copy/paste or tab completion. Table X provides a list of all the input measures we collected for analysis and their exact definitions. 

Furthermore, 
Regular expressions were also used to create shortened descriptions of the patterns of input types used to complete the task with each tool. Table X provides a list of the regular expressions participants used to complete the task with each tool. 
\subsection{Interpretation}
\subsubsection{Exhaustion}
In support of the qualitative data gathered from the surveys on participants' perceived cognitive load, we collect the user input frequencies as a measure of exhaustion due to the total number of interactions required to complete the task. These frequencies are shown for each participant in Tables X-X.
\subsubsection{Dissonance}
To further support the qualitative data gathered from the surveys on participants' perceived cognitive load, we kept track of the specific commands participants used as well as the inputs that make up those commands and patterns of switching between different inputs as a measure of dissonance due to the number of times the participant had to task switch to complete the task. 

\section{Results}
%We compared the tuples of total input type frequencies (k, c, m) to participants' perceived cognitive load ratings and tool preferences. 

%We compared the regular expressions of input types to participants' perceived cognitive load ratings to determine whether there was a correlation with patterns of task switching.

\begin{center}
\begin{tabular}{ | c | c | c | c | c | } 
  \hline
  P1 & Copy/Paste & Keyboard & Click & Page Navigation \\ 
  \hline
  kubectl CLI & 6 & 15 & 8 & 22 \\ 
  \hline
  OpenShift console & 0 & 2 & 77 & 80 \\ 
  \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ | c | c | c | c | c | } 
  \hline
  P2 & Copy/Paste & Keyboard & Click & Page Navigation \\ 
  \hline
  kubectl CLI & 7 & 28 & 20 & 49 \\ 
  \hline
  OpenShift console & 0 & 13 & 145 & 172 \\ 
  \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ | c | c | c | c | c | } 
  \hline
  P3 & Copy/Paste & Keyboard & Click & Page Navigation \\ 
  \hline
  kubectl CLI & 1 & 17 & 1 & 3 \\ 
  \hline
  OpenShift console & 4 & 5 & 142 & 192 \\ 
  \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ | c | c | c | c | c | } 
  \hline
  P4 & Copy/Paste & Keyboard & Click & Page Navigation \\ 
  \hline
  kubectl CLI & 0 & 32 & 2 & 1 \\ 
  \hline
  OpenShift console & 4 & 14 & 133 & 137 \\ 
  \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ | c | c | c | } 
  \hline
  P1 & kubectl CLI & OpenShift console \\ 
  \hline
   &  &  \\ 
  \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ | c | c | c | } 
  \hline
  P2 & kubectl CLI & OpenShift console \\ 
  \hline
   &  &  \\ 
  \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ | c | c | c | } 
  \hline
  P3 & kubectl CLI & OpenShift console \\ 
  \hline
   &  &  \\ 
  \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ | c | c | c | } 
  \hline
  P4 & kubectl CLI & OpenShift console \\ 
  \hline
   &  &  \\ 
  \hline
\end{tabular}
\end{center}

\section{Discussion}

\section{Future Work}

\section{Conclusion}

%Sets the bibliography style to UNSRT and imports the 
%bibliography file "samples.bib".
\bibliographystyle{acm}
\bibliography{references}

\end{document}  