\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex	
\usepackage{array}
\usepackage{amssymb}
\usepackage{todonotes}
\usepackage{caption}

%SetFonts

%SetFonts


\title{User Preferences in Graphical and Textual Interfaces for Kubernetes}
\author{The Authors}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}

\section{Introduction}
%Tool developer culture has established two modalities (text and graphics): how can we be generative and constructive in combining these or offering a new modality in light of their limitations?

%Define: kubernetes, perceived cognitive load, task switching, subtasks...

A challenge we faced was in deciding what metrics to use to compare these tools. We sought to use WhatPulse to measure physical user interaction frequencies and produce mouse movement heatmaps, however we observed errors in the numbers and decided not to use it \cite{}. Furthermore, to what extent do heatmaps let us compare between tools with different input methods? Heatmaps could be a good metric for A/B testing website design, however, they would be a less useful metric for comparing different tools. We assume that these tools are already well designed and instead seek to study the limits that exist within the steady state of design and learning. These modalities will have certain limits and we seek to understand what are they as is to determine how we can improve them. 

Wall clock time is easy to measure and is objective, however, across these tools isn’t really comparable because of the user variance we observed in participant workflows. For example, one participant used the Topology' view to access the logs for each deployment's pods (shown in Figure X) while other participants used the list of pods found in the Project Inventory to access the logs (shown in Figure X). User preference is a better metric, however, it may be too subjective. For example, knowing people prefer a CLI for certain tasks doesn’t help to improve the browser console for those tasks. Therefore, to support the qualitative data gathered in our surveys and study sessions, we also collected low level user interaction data, specifically user input types (keys, clicks, and mouse movements) and how frequently the participant used each user input type in completing the task on both tool interfaces. 

%Since the purpose of our study is not to learn the startup cost of using a tool, we recruited participants that had some familiarity with the tools.
%We employed a classic "Think Aloud" user evaluation method during the sessions to avoid the effects of reactivity \cite{}. 
%The order in which participants used the tools was counter balanced to mitigate learning bias between sessions, with half of the participants using the kubectl CLI first and the other half using the OpenShift console first \cite{}.
%This Kubernetes task was chosen for our study because it is simple enough to be accomplished by a Kubernetes novice in multiple steps or by an expert in fewer steps and it requires participants to perform search and navigation using each tool. 
%Rather than count characters to tally the keyboard input, we chose to count the number of total commands entered to normalize the input frequencies as we are neither counting the duration of the clicks nor the length of the mouse movements.
%As we observed in our study sessions, participants do not consider these inputs as having equal execution difficulty. For example, while half of our participants copy/pasted the pod names from the list of pods in the terminal to get the logs for each pod, the other half used tab completion to get the pod names and execute the command to get the logs.

%The goal of our study is to illuminate user preferences for the flexibility tradeoffs offered by different classes of tools (textual and graphical) for Kubernetes. Furthermore, we seek to understand why users prefer the flexibility tradeoffs offered by different classes of tools in the context of specific interactions. 
We seek to answer the following research questions and determine the validity of our hypotheses:

RQ1. How does tool preference vary by tool type, input type frequency, subtask, and perceived cognitive load?

H1. Preference correlates with input type frequency...

RQ2. How does perceived cognitive load vary by tool type, subtask, and input type switching frequency?

H2. Perceived cognitive load correlates with tool type...

\section{Related Work}
Effects of a hybrid interface and multimodal features for learning how to program \cite{unal2021effects, grafsgaard2014additive}.

Benefits of a textual versus graphical interfaces for learning how to program \cite{dillon2012comparing}.

Measuring perceived cognitive load using the NASA-TLX scale \cite{hart1988development}.

User evaluation methods and the impact of think-aloud on user testing \cite{mcdonald2020impact, ericsson1984protocol, fox2011procedures}. 

Task analysis on the challenges of shell programming \cite{gandhi2020lightening}.

Optimizing visual and textual information and predicting user frustration in search user interfaces \cite{treharne2012optimising, feild2010predicting}.
\todo[inline]{motivate relevance of each and illuminate open research avenues that stem from their findings}


\section{Study Setup}
We conducted a remotely moderated, within subjects user study with 4 participants who had an average of X years of programming experience. All participants belonged to the Hybrid Cloud research group at IBM, regularly used Kubernetes in their work, and had at least some prior experience using the tools. 

Before the study sessions, participants filled out a background survey to provide information about their programming experience, use of Kubernetes, and familiarity with the two tools. Participants downloaded the necessary tools onto their machines (kubectl CLI, OpenShift Console) and either used or was given access to an OpenShift cluster to run Kubernetes. After completing the study, participants filled out a follow up survey and ranked the tools based on their preference. In the follow up survey, participants were also asked to rate the tools based on their perceived cognitive load. To give us a better understanding of preference, participants also rated the quality of each tool and were asked to give reasons for why they either liked or didn’t like using each tool. Participants were also asked to share their recommendations for how each tool could be improved.

During the study sessions, participants completed a Kubernetes task using the kubectl CLI and the OpenShift console. Half of the participants used the kubectl CLI first and the other half used the OpenShift console first. We employed a classic "Think Aloud" user evaluation method. Video and audio from the sessions were recorded using Webex for later transcription and analysis. 

%\subsection{Pilot Testing}
%We conducted pilot testing with two participants to identify any issues in our study design. One participant identified a misleading error in one of the task instructions so we %rewrote it. Initially, we had only provided task hints for the kubectl CLI, and one participant realized that they could not apply these hints to the OpenShift console, so we /%also included task hints for the OpenShift console. We found that both participants were able to complete the task using each tool in under 20 minutes.

The goal of the Kubernetes task was to create an application in a namespace, find the deployment pods that have the string “magic key” in their logs, delete the application, and then delete the namespace. Participants were given a GitHub repo with the application files and task instructions. No time limit was set to complete the task. If participants got stuck during the session, hints specific to each tool interface were provided in hyperlinks below the task instructions. Participants were told to think aloud during the sessions and given a prompt to continue if they fell silent for more than 15 seconds.

\section{Evaluation Methodology}
\subsection{Low Level User Interactions}
The video recordings were manually analyzed to count how frequently participants used the keyboard to enter a command (k), how frequently participants clicked (c), and how frequently participants moved their mouse and/or scrolled to click (m). Participant input type frequencies were stored in tuples as (k, c, m). In the terminal, we counted the total number of commands entered to determine how frequently participants used the keyboard. In the OpenShift console, we counted the number of times participants typed something followed by an enter or click to enter, as the total number of commands entered to determine how frequently participants used the keyboard. In both interfaces, we counted how frequently participants clicked by counting the number of times participants clicked to copy/paste text and the number of times participants clicked a button. In both interfaces, mouse movements were only noted when they were followed by a click to copy/paste or followed by a button click and we counted every time participants scrolled up or down. Table X provides a list of all the input measures we collected for analysis and their exact definitions.

\subsection{High Level User Interactions}
We kept track of the specific commands used by participants, such as how frequently participants used copy/paste or tab completion. Regular expressions were used to create shortened descriptions of the input type patterns used to complete the task with each tool. The regular expressions were composed using the letters that represent the input type (k, c, m). We captured the input type patterns used to complete the following subtasks for each tool interface: command execution, copy/paste, drill down, drill up, and tab completion. Table X provides a list of the regular expressions participants used to complete the task with each tool. 

\subsection{Interpretation}
\subsubsection{Exhaustion}
We collected the user input frequencies as a measure of exhaustion due to the total number of interactions required to complete the task. These frequencies are shown for each participant in Tables 1-4.
\subsubsection{Disruption}
We kept track of the specific commands participants used, the inputs that make up those commands, and patterns of switching between different inputs (composed as regular expressions) as a measure of disruption due to the number of times the participant had to task switch (for example, between typing and mousing) to complete the task. 

\section{Results}
%We compared the tuples of total input type frequencies (k, c, m) to participants' perceived cognitive load ratings and tool preferences. 
%We compared the regular expressions of input types to participants' perceived cognitive load ratings to determine whether there was a correlation with patterns of task switching.

\begin{table}
 \centering
  \begin{tabular}{ | c | c | c | c | c | }
  \hline
  P1 & Copy/Paste & Keyboard & Click & Page Navigation \\ 
  \hline
  kubectl CLI & 6 & 15 & 8 & 22 \\ 
  \hline
  OpenShift console & 0 & 2 & 77 & 80 \\ 
  \hline
  \end{tabular}
 \caption{P1 input frequencies}
\end{table}

\begin{table}
 \centering
  \begin{tabular}{ | c | c | c | c | c | } 
  \hline
  P2 & Copy/Paste & Keyboard & Click & Page Navigation \\ 
  \hline
  kubectl CLI & 7 & 28 & 20 & 49 \\ 
  \hline
  OpenShift console & 0 & 13 & 145 & 172 \\ 
  \hline
  \end{tabular}
 \caption{P2 input frequencies}
\end{table}

\begin{table}
 \centering
  \begin{tabular}{ | c | c | c | c | c | } 
  \hline
  P3 & Copy/Paste & Keyboard & Click & Page Navigation \\ 
  \hline
  kubectl CLI & 1 & 17 & 1 & 3 \\ 
  \hline
  OpenShift console & 4 & 5 & 142 & 192 \\ 
  \hline
  \end{tabular}
 \caption{P3 input frequencies}
\end{table}

\begin{table}
 \centering
  \begin{tabular}{ | c | c | c | c | c | } 
  \hline
  P4 & Copy/Paste & Keyboard & Click & Page Navigation \\ 
  \hline
  kubectl CLI & 0 & 32 & 2 & 1 \\ 
  \hline
  OpenShift console & 4 & 14 & 133 & 137 \\ 
  \hline
  \end{tabular}
 \caption{P4 input frequencies}
\end{table}

\begin{center}
\begin{tabular}{ | c | c | c | } 
  \hline
  P1 & kubectl CLI & OpenShift console \\ 
  \hline
   &  &  \\ 
  \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ | c | c | c | } 
  \hline
  P2 & kubectl CLI & OpenShift console \\ 
  \hline
   &  &  \\ 
  \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ | c | c | c | } 
  \hline
  P3 & kubectl CLI & OpenShift console \\ 
  \hline
   &  &  \\ 
  \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ | c | c | c | } 
  \hline
  P4 & kubectl CLI & OpenShift console \\ 
  \hline
   &  &  \\ 
  \hline
\end{tabular}
\end{center}

\section{Design Implications}

\section{Future Work}

\section{Conclusion}

%Sets the bibliography style to UNSRT and imports the 
%bibliography file "samples.bib".
\bibliographystyle{acm}
\bibliography{references}

\end{document}  